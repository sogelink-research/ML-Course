{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify buildings from DSM data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open this notebook with Google Colab, click on the following link:\n",
    "\n",
    "[Open in Google Colab](https://colab.research.google.com/github/sogelink-research/ML-Course/blob/main/notebooks/1-introduction_shorter/building_classification.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary stuff for Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary files if running in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    import urllib.request\n",
    "    import zipfile\n",
    "    from pathlib import Path\n",
    "    from shutil import copy, copytree, rmtree\n",
    "\n",
    "    # Download the GitHub repository\n",
    "    zip_path = Path(\"simple_model.zip\")\n",
    "    directory_path = Path(\".\")\n",
    "    initial_simple_model_path = Path(\n",
    "        \"ML-Course-main/notebooks/1-introduction_shorter/simple_model\"\n",
    "    )\n",
    "    simple_model_path = Path(\"simple_model\")\n",
    "\n",
    "    url = \"https://github.com/sogelink-research/ML-Course/archive/refs/heads/main.zip\"\n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "\n",
    "    # Unzip the repository\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(directory_path)\n",
    "\n",
    "    # Take the files from the simple model\n",
    "    copytree(\n",
    "        initial_simple_model_path,\n",
    "        simple_model_path,\n",
    "        copy_function=lambda s, d: not Path(d).exists() and copy(s, d),\n",
    "        dirs_exist_ok=True,\n",
    "    )\n",
    "\n",
    "    # Take the requirements\n",
    "    initial_requirements_path = Path(\n",
    "        \"ML-Course-main/notebooks/1-introduction_shorter/requirements.txt\"\n",
    "    )\n",
    "    requirements_path = Path(\"requirements.txt\")\n",
    "    copy(initial_requirements_path, requirements_path)\n",
    "\n",
    "    # Clean the rest\n",
    "    zip_path.unlink()\n",
    "    rmtree(Path(\"ML-Course-main\"))\n",
    "\n",
    "    def get_files(path: Path, extensions: list[str]):\n",
    "        all_files = []\n",
    "        for ext in extensions:\n",
    "            all_files.extend(path.glob(f\"*.{ext}\"))\n",
    "        return all_files\n",
    "\n",
    "    for file_path in get_files(directory_path, [\"py\", \"just\"]):\n",
    "        file_path.unlink()\n",
    "\n",
    "    print(\"Downloaded the necessary files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the necessary packages if running in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from simple_model.bbox import BboxInt\n",
    "from simple_model.dataloader import ImagesLoader\n",
    "from simple_model.dataparse import download_all, tile_image\n",
    "from simple_model.nn import SegmentationConvolutionalNetwork, get_new_model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the parameters\n",
    "\n",
    "| Name | Type | Possible values | Description | Advice |\n",
    "| ---- | ---- | --------------- | ----------- | ------------ |\n",
    "| minx, maxy, maxx, miny | **integers** | Coordinates in EPSG:7415 in the Netherlands. | Coordinates of the bounding box of the area that will be used to train the model | Larger area means more data so potentially better final model but longer to train. |\n",
    "| filter_buildings | boolean | Any | Whether to filter out small buildings in the dataset (area < 30m² and \"gebruiksdoel\" undefined). | Keep it true. |\n",
    "| image_size | integer | > 0 | Size of the images processed by the model (they will be (image_size×image_size)). | Since the image size is divided multiple times by 2, it is necessary to use a multiple of 2 with high-enough multiplicity. |\n",
    "| nodata | float | Any | Value to replace NO_DATA with. | This value will be used to replace any missing pixel, mainly water but also potential holes in the data in buildings, roads, etc. |\n",
    "| encoder_channels | list of integers | > 0 | Number of channels in the successive steps of the encoder. This also defines the number of steps. | More channels mean larger model with longer training and running time, but potentially better capabilities. Usually, the values inside are increasing, to capture more and more features at each step. |\n",
    "| layers_downsample | integer | > 0 | Number of convolutional layers each step the encoder. | More layers should be able to extract more information, at the cost of a larger model. |\n",
    "| layers_upsample | integer | > 0 | Number of convolutional layers each step the decoder. | More layers should be able to better combiner information, at the cost of a larger model. |\n",
    "| batch_size | integer | > 0 | Size of the batches (number of images processed at once). | Larger batches mean more images processed before each training step, meaning less steps per epoch, but potentially \"better\" steps thanks to processing more images. The available memory of the GPU, in relation to the other parameters, define the maximum possible value for a given model and a given GPU. |\n",
    "| train_proportion | float | Between 0 and 1 | Proportion of the data used for training. The rest is used for validation during training, to assess the current model on data that it wasn't trained on. | You usually want to use most of the data for training, but still want enough to get a decent evaluation. |\n",
    "| optimizer_type | string | \"adam\" or \"sgd\" | Type of optimizer used to train the model. | Adam is usually a good choice for most models. |\n",
    "| initial_learning_rate | float | > 0 | Initial learning rate of the optimizer. | A larger value means faster training but also more risk of not converging to a good solution. The value highly depend on the other parameters, like the size of the model or the batch size, but usually values from 0.0001 to 0.01 are used. |\n",
    "| max_epochs | integer | > 0 | Maximum number of iterations to train the model. Each iteration goes through the whole dataset. | You usually want this value to be high enough for the model to be as optimised as possible. Here you can interrupt the training when you want, but this can be an important parameter when training a model without monitoring the process. |\n",
    "| stop_early_after | integer | > 0 | Number of epochs without improvement before stopping the training. Prevents running for all the given epochs when the model doesn't seem to be improving any more. | A larger value prevents stopping too early if the model could still improve despite temporarily not improving. But it also means that the training won't stop as early if the model is really stuck. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the data online and select one or multiple areas to work on, you can use:\n",
    "\n",
    "- This website to view the data and select coordinates (using the \"Measurements\" tool) in the bottom right: <https://app.ellipsis-drive.com/view?pathId=78080fff-8bcb-4258-bb43-be9de956b3e0>\n",
    "- This website to convert coordinates to EPSG:7415: <https://epsg.io/transform#s_srs=4326&t_srs=7415&x=NaN&y=NaN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ Data selection ------------------------------ #\n",
    "minx_miny_maxx_maxy = [\n",
    "    (None, None, None, None),\n",
    "    (None, None, None, None),\n",
    "    ...,\n",
    "]  # Coordinates of the areas of interest (minx, maxy, maxx, miny)\n",
    "filter_buildings = True  # True to filter small buildings\n",
    "\n",
    "# ----------------------------- Data preparation ----------------------------- #\n",
    "image_size = None  # Size of the images processed by the model\n",
    "nodata = None  # Value to replace nodata with\n",
    "\n",
    "# ----------------------------- Model parameters ----------------------------- #\n",
    "encoder_channels = [None, None, ...]  # Number of channels in the encoder and decoder\n",
    "layers_downsample = None  # Number of convolutional layers in the encoder\n",
    "layers_upsample = None  # Number of convolutional layers in the decoder\n",
    "\n",
    "# ---------------------------- Training parameters --------------------------- #\n",
    "batch_size = None  # Size of the batches (number of images processed at once)\n",
    "train_proportion = None  # Proportion of the data used for training\n",
    "\n",
    "optimizer_type = None  # Type of optimizer to use (\"adam\" or \"sgd\")\n",
    "initial_learning_rate = None  # Initial learning rate for the optimizer\n",
    "max_epochs = None  # Maximum number of iterations to train the model\n",
    "stop_early_after = (\n",
    "    None  # Number of epochs without improvement before stopping the training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the bounding box\n",
    "# bbox = BboxInt(minx, maxy, maxx, miny, True)\n",
    "\n",
    "# Create the bounding boxes\n",
    "bboxes = [\n",
    "    BboxInt(minx, miny, maxx, maxy, True)\n",
    "    for (minx, miny, maxx, maxy) in minx_miny_maxx_maxy\n",
    "]\n",
    "\n",
    "# Download the data\n",
    "main_data_folder = Path(\"data\")\n",
    "main_models_folder = Path(\"models\")\n",
    "\n",
    "main_data_folder.mkdir(parents=True, exist_ok=True)\n",
    "main_models_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "data_folders = []\n",
    "full_image_paths = []\n",
    "full_mask_paths = []\n",
    "for bbox in bboxes:\n",
    "    # Download the data\n",
    "    print(f\"Downloading data for {bbox}\")\n",
    "    data_folder, full_image_path, full_mask_path = download_all(\n",
    "        bbox, main_data_folder, filter_buildings\n",
    "    )\n",
    "    data_folders.append(data_folder)\n",
    "    full_image_paths.append(full_image_path)\n",
    "    full_mask_paths.append(full_mask_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tile the images and masks\n",
    "images_paths = []\n",
    "masks_paths = []\n",
    "for full_image_path in full_image_paths:\n",
    "    images_path = tile_image(full_image_path, tile_size=image_size)\n",
    "    images_paths.append(images_path)\n",
    "\n",
    "for full_mask_path in full_mask_paths:\n",
    "    masks_path = tile_image(full_mask_path, tile_size=image_size)\n",
    "    masks_paths.append(masks_path)\n",
    "\n",
    "# Create the images loader\n",
    "image_shape = (image_size, image_size)\n",
    "images_loader = ImagesLoader(image_shape=image_shape, nodata=nodata)\n",
    "for data_folder, images_path, masks_path in zip(\n",
    "    data_folders, images_paths, masks_paths\n",
    "):\n",
    "    images_loader.load_data(data_folder, images_path, masks_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = get_new_model_name()\n",
    "model_folder = main_models_folder / model_name\n",
    "model_folder.mkdir(parents=True, exist_ok=True)\n",
    "model = SegmentationConvolutionalNetwork(\n",
    "    image_shape=image_shape,\n",
    "    encoder_channels=encoder_channels,\n",
    "    layers_downsample=layers_downsample,\n",
    "    layers_upsample=layers_upsample,\n",
    "    input_channels=1,\n",
    "    model_folder=model_folder,\n",
    "    data_folders=data_folders,\n",
    ")\n",
    "\n",
    "# # Better speed for CPU\n",
    "# torch.compile(model)\n",
    "model.plot_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloaders with proper configuration\n",
    "train_val_dataloaders = images_loader.get_dataloaders(\n",
    "    batch_size=batch_size, train_proportion=train_proportion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.train_model(\n",
    "    dataloaders=train_val_dataloaders,\n",
    "    max_epochs=max_epochs,\n",
    "    optimizer_type=optimizer_type,\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    stop_early_after=stop_early_after,\n",
    "    save_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and save the model predictions\n",
    "output_folder = model_folder / \"output\"\n",
    "model.save_predictions(\n",
    "    images_loader=images_loader,\n",
    "    dataloaders=train_val_dataloaders,\n",
    "    output_folder=output_folder,\n",
    ")\n",
    "\n",
    "# Compute and save the model metrics\n",
    "metrics_folder = model_folder / \"metrics\"\n",
    "model.save_metrics(\n",
    "    images_loader=images_loader,\n",
    "    dataloaders=train_val_dataloaders,\n",
    "    metrics_folder=metrics_folder,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zip the results to download locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the model folder\n",
    "if IN_COLAB:\n",
    "    import shutil\n",
    "\n",
    "    shutil.make_archive(\n",
    "        model_folder, \"zip\", root_dir=model_folder.parent, base_dir=model_folder.name\n",
    "    )\n",
    "    print(f\"Model saved at {model_folder}.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a previously trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_folder = Path(\"models/250409_232909\")\n",
    "# model = SegmentationConvolutionalNetwork.load_model(model_folder=model_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
