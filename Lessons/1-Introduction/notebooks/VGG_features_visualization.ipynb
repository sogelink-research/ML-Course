{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COKY0TfYkvtF"
   },
   "source": [
    "# HW02 - Visualizing features of a pretrained VGG\n",
    "\n",
    "In this homework, we are going to try to visualize what neurons encode through optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9YfPu26lphB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import urllib.request\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using gpu: %s \" % torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "For the background of ipywidget\n",
    "\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOa1nsT1SeiK"
   },
   "source": [
    "First, load the pretrained VGG model with torchvision, and print the architecture of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C08sMqFVl72r"
   },
   "outputs": [],
   "source": [
    "model_vgg = models.vgg16(weights=\"DEFAULT\", progress=True)\n",
    "print(model_vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1-7GFawSwUd"
   },
   "source": [
    "# Part A - Visualizing the convolution filters\n",
    "\n",
    "First, plot all the filters for the red channel of the first convolutional layer (there should be 64 filters in total). Can you find filters that seem to encode edges? Is this method useful for other layers?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkLkXPYsl-EH"
   },
   "outputs": [],
   "source": [
    "first_conv_layer = model_vgg.features[0]\n",
    "red_channel_filters = first_conv_layer.weight.data[:, 0]\n",
    "filters_count = len(red_channel_filters)\n",
    "plot_cols_rows = int(np.ceil(np.sqrt(filters_count)))\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(filters_count):\n",
    "    plt.subplot(plot_cols_rows, plot_cols_rows, i + 1)\n",
    "    plt.imshow(red_channel_filters[i], cmap=\"gray\")\n",
    "    plt.title(f\"Filter {i}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #0d0; padding: 10px; width: auto\">\n",
    "\n",
    "There are many filters which seem to encode edges or close to edges in the first convolutional layer `model_vgg.features[0]`:\n",
    "\n",
    "- Vertical edges: filters 0, 6, 17, 26...\n",
    "- Horizontal edges: filters 5, 9, 21...\n",
    "- And even diagonal edges: filters 8, 20, 22...\n",
    "\n",
    "Then, if we look for example at the third convolutional layer `model_vgg.features[5]`, we don't really see anymore that many filters which seem to encode edges. From a human point of view, it is hard to understand what an individual filter from this layer is optimized for.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfFjvNYIUDrz"
   },
   "source": [
    "# Part B - Visualizing channel activations through image optimization\n",
    "\n",
    "## B.1 - First implementation\n",
    "\n",
    "Create a module `ChannelActivation(layer, channel)` that returns the average activation (i.e. output value) of channel `channel` of layer `layer` of the VGG features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iy0lfWYibYhp"
   },
   "outputs": [],
   "source": [
    "class ChannelActivation(nn.Module):\n",
    "    def __init__(self, layer, channel):\n",
    "        super(ChannelActivation, self).__init__()\n",
    "        self.layer = layer\n",
    "        self.channel = channel\n",
    "        self.model_vgg_gpu = model_vgg.to(device)\n",
    "        self.model_vgg_gpu.eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x\n",
    "        for i in range(self.layer + 1):\n",
    "            output = self.model_vgg_gpu.features[i](output)\n",
    "        output_channel = output[:, self.channel]\n",
    "        mean_activation = torch.mean(output_channel)\n",
    "        return mean_activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFcMVNs-bZL0"
   },
   "source": [
    "Our objective is to find which patterns are recognized by a given channel. To do so, we will follow the approach of [this Distill article](https://distill.pub/2017/feature-visualization/) and find images that lead to the highest possible channel activation.\n",
    "\n",
    "First, create a random (colored) image of size 128x128, initialized with value at random between 0.4 and 0.6 (i.e. grey + small perturbation). Then, perform 200 steps of Adam (with lr=0.01) to maximize the activation of channel 4 of layer 1. Plot the image after 0, 10, 50, 100 and 200 iterations. You should see a pink saturated image with several horizontal lines, indicating that the channel probably recognizes horizontal edges.\n",
    "\n",
    "**NB1:** Careful, by default, optimizers minimize their objective, not maximize it!\n",
    "\n",
    "**NB2:** The parameters given to an optimizer should be on the cpu. If you use a gpu, you thus need to keep two versions of the image: 1) a cpu version given to the optimizer, and 2) a gpu version, created at each iteration of the optimization, and used to compute the gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_activation_optimization(layer, channel, clip=False):\n",
    "    input_image_cpu = (0.2 * torch.rand(1, 3, 128, 128) + 0.4).requires_grad_()\n",
    "    saved_steps = [0, 10, 50, 100, 200]\n",
    "    saved_images = [np.copy(input_image_cpu.detach())]\n",
    "\n",
    "    model = ChannelActivation(layer, channel).to(device)\n",
    "    optimizer = torch.optim.Adam([input_image_cpu], lr=0.01, maximize=True)\n",
    "\n",
    "    model.eval()\n",
    "    steps = 200\n",
    "    for step in tqdm(range(1, steps + 1), desc=\"step\"):\n",
    "        input_image_gpu = input_image_cpu.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        mean_activation = model(input_image_gpu).to(device)\n",
    "        mean_activation.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Clip the value of the pixels to avoid saturation\n",
    "        if clip:\n",
    "            input_image_cpu.data = torch.clamp(input_image_cpu.data, 0.0, 1.0)\n",
    "\n",
    "        # Store the new image\n",
    "        if step == saved_steps[len(saved_images)]:\n",
    "            saved_images.append(np.copy(input_image_cpu.detach()))\n",
    "\n",
    "    instances = len(saved_steps)\n",
    "    cols = int(np.ceil(np.sqrt(instances)))\n",
    "    rows = (cols - 1) // 2 + 1\n",
    "    plt.figure(figsize=(15, 15 * rows / cols))\n",
    "    for i, (step, image) in enumerate(zip(saved_steps, saved_images)):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(image[0].transpose(1, 2, 0))\n",
    "        plt.title(f\"Step {step}\")\n",
    "    plt.show()\n",
    "\n",
    "    return input_image_cpu[0].detach().numpy().transpose(1, 2, 0)\n",
    "\n",
    "\n",
    "layer = 1\n",
    "channel = 4\n",
    "activation_image = image_activation_optimization(layer, channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dk-_lzkY7g0h"
   },
   "source": [
    "## B.2 - Improving stability with clipping and normalization\n",
    "\n",
    "Compute the highest and lowest values of the image. What is the issue?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ur663Vfj_0Xd"
   },
   "outputs": [],
   "source": [
    "image_min_value = np.min(activation_image)\n",
    "image_max_value = np.max(activation_image)\n",
    "\n",
    "print(f\"The lowest value of the image is {image_min_value}\")\n",
    "print(f\"The highest value of the image is {image_max_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #0d0; padding: 10px; width: auto\">\n",
    "\n",
    "The issue is that the model doesn't limit itself to values between 0 and 1.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gG8Z6tzAi0F9"
   },
   "source": [
    "To avoid (over) saturation, clip the image pixels to $[0.2,0.8]$ after each optimization step using `input_image.data = input_image.data.clip(0.2, 0.8)`. You should now see several clear horizontal lines in a blue background.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AKhdohKYi1Im"
   },
   "outputs": [],
   "source": [
    "layer = 1\n",
    "channel = 4\n",
    "image_cpu = image_activation_optimization(layer, channel, clip=True)\n",
    "\n",
    "image_min_value = np.min(image_cpu)\n",
    "image_max_value = np.max(image_cpu)\n",
    "\n",
    "print(f\"The lowest value of the image is {image_min_value}\")\n",
    "print(f\"The highest value of the image is {image_max_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hj1njbSf5Ve"
   },
   "source": [
    "One issue with our current code, is that VGG was trained on **normalized** images, and thus is not adapted to our input image. To normalize the image, we will use **transforms**.\n",
    "\n",
    "Create a function `create_activation_image(layer, channel, transform=None, image_size=128, show_steps=False)` that maximizes the corresponding channel activation on an image of size `image_size`, and first applies `transform` to the image before computing the gradient of the activation. The function should return the final image after 200 steps, and plot intermediate images for the steps 0,10,50,100,200 if `show_steps=True`.\n",
    "\n",
    "Then, test your function with `transform=transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])`. Is this better? You should now see a horizontal pattern with lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5qMxoYpCeN2"
   },
   "outputs": [],
   "source": [
    "def create_activation_image(\n",
    "    layer, channel, transform=None, image_size=128, show_steps=False, progress_bar=True\n",
    "):\n",
    "    input_image_cpu = (\n",
    "        0.2 * torch.rand(1, 3, image_size, image_size) + 0.4\n",
    "    ).requires_grad_()\n",
    "    if show_steps:\n",
    "        saved_steps = [0, 10, 50, 100, 200]\n",
    "        saved_images = [np.copy(input_image_cpu.detach())]\n",
    "\n",
    "    model = ChannelActivation(layer, channel).to(device)\n",
    "    optimizer = torch.optim.Adam([input_image_cpu], lr=0.01, maximize=True)\n",
    "\n",
    "    model.eval()\n",
    "    steps = 200\n",
    "    if progress_bar:\n",
    "        iterator = tqdm(range(1, steps + 1), desc=\"step\")\n",
    "    else:\n",
    "        iterator = range(1, steps + 1)\n",
    "    for step in iterator:\n",
    "        input_image_gpu = transform(input_image_cpu.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        mean_activation = model(input_image_gpu).to(device)\n",
    "        mean_activation.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Clip the value of the pixels to avoid saturation\n",
    "        input_image_cpu.data = torch.clamp(input_image_cpu.data, 0.2, 0.8)\n",
    "\n",
    "        # Store the new image\n",
    "        if show_steps and (step == saved_steps[len(saved_images)]):\n",
    "            saved_images.append(np.copy(input_image_cpu.detach()))\n",
    "\n",
    "    # Show the image at different steps\n",
    "    if show_steps:\n",
    "        instances = len(saved_steps)\n",
    "        cols = int(np.ceil(np.sqrt(instances)))\n",
    "        rows = (cols - 1) // 2 + 1\n",
    "        plt.figure(figsize=(15, 15 * rows / cols))\n",
    "        for i, (step, image) in enumerate(zip(saved_steps, saved_images)):\n",
    "            plt.subplot(rows, cols, i + 1)\n",
    "            plt.imshow(image[0].transpose(1, 2, 0))\n",
    "            plt.title(f\"Step {step}\")\n",
    "        plt.show()\n",
    "\n",
    "    return input_image_cpu[0].detach().numpy().transpose(1, 2, 0)\n",
    "\n",
    "\n",
    "layer = 1\n",
    "channel = 4\n",
    "transform = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "image_size = 256\n",
    "show_steps = True\n",
    "activation_image = create_activation_image(\n",
    "    layer, channel, transform, image_size, show_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzpIF7qrEFQD"
   },
   "source": [
    "Now test your function on channel 0 of layer 20. The pattern that appears should vagely resemble fish scales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dcx-Y7dLEChU"
   },
   "outputs": [],
   "source": [
    "layer = 20\n",
    "channel = 0\n",
    "transform = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "image_size = 256\n",
    "show_steps = True\n",
    "activation_image = create_activation_image(\n",
    "    layer, channel, transform, image_size, show_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nX5Hs0MFIh4"
   },
   "source": [
    "## B.3 - Transformation robustness\n",
    "\n",
    "Large neural network are prone to adversarial attacks, i.e. a small well-crafted additive noise can dramatically change the output of the model, and thus lead to incorrect classification. For our purpose, this is an issue, as the optimization algorithm may find such very specific noise instead of more valuable visual patterns.\n",
    "\n",
    "To avoid this issue and further improve our images, we are thus going to apply small random perturbations to the image before computing the gradient. This will prevent the optimizer from optimizing the noise, and overall increase the stability of our process.\n",
    "\n",
    "To do so, add a composition of several transforms (before the normalization):\n",
    "\n",
    "1.  A small pixel noise with `transforms.Lambda(lambda x: x + 0.001 * (2 * torch.rand_like(x) - 1))`\n",
    "2.  A random affine transform with `transforms.RandomAffine(degrees=5, translate=(0.1,0.1), scale=(0.9,1.1))`\n",
    "3.  A random crop of size 96 (to reduce the size of the image)\n",
    "4.  Random local fluctations with `transforms.ElasticTransform(alpha=50.)`\n",
    "\n",
    "Compare the activation images with and without these random transformations. Is the pattern more visible?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TR_Sh4Df4cI"
   },
   "outputs": [],
   "source": [
    "layer = 1\n",
    "channel = 4\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.Lambda(lambda x: x + 0.001 * (2 * torch.rand_like(x) - 1)),\n",
    "        transforms.RandomAffine(degrees=5, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.RandomCrop(192),\n",
    "        transforms.ElasticTransform(alpha=50.0),\n",
    "    ]\n",
    ")\n",
    "image_size = 256\n",
    "show_steps = True\n",
    "activation_image = create_activation_image(\n",
    "    layer, channel, transform, image_size, show_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 20\n",
    "channel = 0\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.Lambda(lambda x: x + 0.001 * (2 * torch.rand_like(x) - 1)),\n",
    "        transforms.RandomAffine(degrees=5, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.RandomCrop(192),\n",
    "        transforms.ElasticTransform(alpha=50.0),\n",
    "    ]\n",
    ")\n",
    "image_size = 256\n",
    "show_steps = True\n",
    "\n",
    "activation_image = create_activation_image(\n",
    "    layer, channel, transform, image_size, show_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wy2EYMBmm-z3"
   },
   "source": [
    "To see what the transformation is doing to an image, apply the random transformations (without normalization) to the following simple image, and show 5 randomly transformed images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bwkgeQmm-LK"
   },
   "outputs": [],
   "source": [
    "sample_image = 0.3 * torch.ones(3, 256, 256)\n",
    "sample_image[0, :, 40:80] += 0.7\n",
    "sample_image[1, 10:20, :] += 0.5\n",
    "sample_image[2, 150:, :] += 0.5\n",
    "plt.imshow(transforms.functional.to_pil_image(sample_image))\n",
    "plt.show()\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Lambda(lambda x: x + 0.001 * (2 * torch.rand_like(x) - 1)),\n",
    "        transforms.RandomAffine(degrees=5, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.RandomCrop(192),\n",
    "        transforms.ElasticTransform(alpha=50.0),\n",
    "    ]\n",
    ")\n",
    "\n",
    "instances = 6\n",
    "cols = int(np.ceil(np.sqrt(instances)))\n",
    "rows = (cols - 1) // 2 + 1\n",
    "plt.figure(figsize=(15, 15 * rows / cols))\n",
    "for i in range(instances):\n",
    "    transformed_sample_image = transforms.functional.to_pil_image(\n",
    "        transform(sample_image)\n",
    "    )\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    plt.imshow(transformed_sample_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3rQg057IUiR"
   },
   "source": [
    "## B.4 - Final visualization\n",
    "\n",
    "Finally, show the activation images for the first 5 channels of layers [1, 10, 20, 30]. You should be able to see a gradual complexification of the patterns.\n",
    "\n",
    "**PS1:** Our method seems unable to find meaningful patterns for the last layer. One issue is probably that the random crop imposes that all regions on the image look similar (as they all should have a high channel activation), thus preventing larger and more complex patterns to emerge from the optimization.\n",
    "\n",
    "**PS2:** You can also try other layers and channels to find interesting patterns!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6lYy9SoIUwJ"
   },
   "outputs": [],
   "source": [
    "layers = [1, 10, 20, 30]\n",
    "channels = [0, 1, 2, 3, 4, 5]\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.Lambda(lambda x: x + 0.001 * (2 * torch.rand_like(x) - 1)),\n",
    "        transforms.RandomAffine(degrees=5, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.RandomCrop(96),\n",
    "        transforms.ElasticTransform(alpha=50.0),\n",
    "    ]\n",
    ")\n",
    "image_size = 256\n",
    "show_steps = False\n",
    "\n",
    "# Compute the activation images\n",
    "activation_images = np.zeros((len(layers), len(channels), image_size, image_size, 3))\n",
    "for i, layer in enumerate(tqdm(layers, position=0, desc=\"layer\", colour=\"green\")):\n",
    "    for j, channel in enumerate(\n",
    "        tqdm(channels, position=1, desc=\"channel\", leave=\"False\", colour=\"red\")\n",
    "    ):\n",
    "        activation_images[i, j] = create_activation_image(\n",
    "            layer, channel, transform, image_size, show_steps, progress_bar=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the images\n",
    "for i, layer in enumerate(layers):\n",
    "    instances = len(channels)\n",
    "    cols = int(np.ceil(np.sqrt(instances)))\n",
    "    rows = (cols - 1) // 2 + 1\n",
    "    plt.figure(figsize=(15, 15 * rows / cols))\n",
    "    plt.suptitle(f\"Layer {layer}\")\n",
    "    for j, channel in enumerate(channels):\n",
    "        plt.subplot(rows, cols, j + 1)\n",
    "        plt.imshow(activation_images[i, j])\n",
    "        plt.title(f\"Channel {channel}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.vgg16(weights=\"DEFAULT\", progress=True)\n",
    "model = models.convnext_tiny(weights=\"DEFAULT\", progress=True)\n",
    "model.layers = model.features\n",
    "print(model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to the ImageNet class labels JSON file\n",
    "url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "\n",
    "# Download the file and load the class labels\n",
    "class_labels = []\n",
    "with urllib.request.urlopen(url) as f:\n",
    "    class_labels = [line.decode(\"utf-8\").strip() for line in f.readlines()]\n",
    "\n",
    "# Print the first 10 class labels\n",
    "print(\"First 10 class labels:\", class_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_activation_image\n",
    "\n",
    "layers = [len(model.layers) - 1]\n",
    "channels = list(range(4))\n",
    "channels_names = [class_labels[i] for i in channels]\n",
    "steps = 400\n",
    "lr = 0.04\n",
    "show_steps = True\n",
    "input_shape = (3, 236, 236)\n",
    "\n",
    "# activation_images_transform = None\n",
    "activation_images_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.Lambda(lambda x: x + 0.001 * (2 * torch.rand_like(x) - 1)),\n",
    "        transforms.RandomAffine(degrees=5, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ElasticTransform(alpha=50.0),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Compute the activation images\n",
    "activation_images = np.zeros((len(layers), len(channels), *input_shape))\n",
    "for i, layer in enumerate(tqdm(layers, position=0, desc=\"layer\", colour=\"green\")):\n",
    "    for j, channel in enumerate(\n",
    "        tqdm(channels, position=1, desc=\"channel\", colour=\"red\")\n",
    "    ):\n",
    "        activation_images[i, j] = create_activation_image(\n",
    "            model=model,\n",
    "            layer=layer,\n",
    "            channel=channel,\n",
    "            input_mean=0,\n",
    "            input_std=1,\n",
    "            steps=steps,\n",
    "            lr=lr,\n",
    "            show_steps=show_steps,\n",
    "            transform=activation_images_transform,\n",
    "            input_shape=input_shape,\n",
    "            progress_bar=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the images\n",
    "for i, layer in enumerate(layers):\n",
    "    instances = len(channels)\n",
    "    cols = int(np.ceil(np.sqrt(instances)))\n",
    "    rows = int(np.ceil(instances / cols))\n",
    "    plt.figure(figsize=(15, 15 * rows / cols))\n",
    "    plt.suptitle(f\"Layer {layer}\")\n",
    "    for j, (channel, channel_name) in enumerate(zip(channels, channels_names)):\n",
    "        plt.subplot(rows, cols, j + 1)\n",
    "        plt.imshow(activation_images[i, j].transpose(1, 2, 0).squeeze(), cmap=\"gray\")\n",
    "        plt.title(channel_name)\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import json\n",
    "import urllib.request\n",
    "\n",
    "# Load the pre-trained VGG16 model\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# URL to the ImageNet class labels JSON file\n",
    "url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "\n",
    "# Download the file and load the class labels\n",
    "class_labels = []\n",
    "with urllib.request.urlopen(url) as f:\n",
    "    class_labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Print the first 10 class labels\n",
    "print(\"First 10 class labels:\", class_labels[:10])\n",
    "\n",
    "# Example: Predict the class of a random input tensor\n",
    "input_tensor = torch.randn(1, 3, 224, 224)  # Random input tensor\n",
    "output = vgg16(input_tensor)\n",
    "_, predicted_idx = torch.max(output, 1)\n",
    "predicted_class = class_labels[predicted_idx.item()]\n",
    "\n",
    "print(\"Predicted class:\", predicted_class)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
